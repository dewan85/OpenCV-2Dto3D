// Taken from:
// https://stackoverflow.com/questions/44104633/transforming-2d-image-coordinates-to-3d-world-coordinates-with-z-0
// https://github.com/IndrajeetDatta/Extrinsics
// http://answers.opencv.org/question/62779/image-coordinate-to-world-coordinate-opencv/
// https://stackoverflow.com/questions/12299870/computing-x-y-coordinate-3d-from-image-point
#include "opencv2/opencv.hpp"
#include <stdio.h>
#include <iostream>
#include <sstream>
#include <math.h>
#include <cstdlib>

cv::Mat cameraMatrix, distCoeffs, rotationVector, rotationMatrix,
		translationVector, invR_x_invM_x_uv1, invR_x_tvec, wcPoint;
std::vector<cv::Point2d> image_points;
std::vector<cv::Point3d> world_points;
double Z;

int main(int argc, char *argv[]) {
	// Intrinsics is already calculated
	cv::FileStorage intrin("intrinsics.yml", cv::FileStorage::READ);
	cv::FileStorage extrin("extrinsics.yml", cv::FileStorage::READ);

	intrin["camera_matrix"] >> cameraMatrix;
	intrin["distortion_coefficients"] >> distCoeffs;
	extrin["rotationmatrix"] >> rotationMatrix;
	extrin["translationvector"] >> translationVector;
	std::cerr << "Camera Matrix: " << cameraMatrix << std::endl << std::endl;
	std::cerr << "Distortion Coefficients: " << distCoeffs << std::endl;
	std::cerr << "Rotation Matrix: " << rotationMatrix << std::endl;
	std::cerr << "Translation Vector: " << translationVector << std::endl;

	// Example usage: transform the third point:
	// screen coordinates (331, 308) to world coordinates should produce (1.775, 4.620, Z).
	// we use the standard focal_distance=1 (generated by Mat::ones) and will define Z=0.
	cv::Mat screenCoordinates = cv::Mat::ones(3, 1, cv::DataType<double>::type);
	screenCoordinates.at<double>(0, 0) = 331;
	screenCoordinates.at<double>(1, 0) = 308;
	if (argc > 2) {
		screenCoordinates.at<double>(0, 0) = atoi(argv[1]);
		screenCoordinates.at<double>(1, 0) = atoi(argv[2]);
	}
	screenCoordinates.at<double>(2, 0) = 1; // f=1
	std::cerr << "Camera Coordinates:" << screenCoordinates << std::endl
			<< std::endl;

	// Hypothesis ground:
	Z = 0;

	// s and point calculation, described here:
	// https://stackoverflow.com/questions/12299870/computing-x-y-coordinate-3d-from-image-point
	invR_x_invM_x_uv1 = rotationMatrix.inv() * cameraMatrix.inv()
			* screenCoordinates;
	invR_x_tvec = rotationMatrix.inv() * translationVector;
	wcPoint = (Z + invR_x_tvec.at<double>(2, 0))
			/ invR_x_invM_x_uv1.at<double>(2, 0) * invR_x_invM_x_uv1
			- invR_x_tvec;
	cv::Point3f worldCoordinates(wcPoint.at<double>(0, 0),
			wcPoint.at<double>(1, 0), wcPoint.at<double>(2, 0));
	std::cerr << "World Coordinates" << worldCoordinates << std::endl
			<< std::endl;

	std::cout << "[" << screenCoordinates.at<double>(0, 0) << ","
			<< screenCoordinates.at<double>(1, 0) << "] -> ["
			<< worldCoordinates.x << "," << worldCoordinates.y << "]"
			<< std::endl;
	return 0;
}
